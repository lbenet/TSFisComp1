{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Envío del PR inicial: 24.08.2018** \n",
    "\n",
    "**Aceptación del PR: 02.09.2018** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "- Escriban una función `newton` que, a partir de `f`, `fprime` y `x0` dados, obtenga una de las raices de la ecuación. Comprueben que funciona con $f(x)=x^2 -2$ y $f'(x) = 2x$. Tengan suficiente cuidado para que no haya ningún tipo de inestabilidad de tipo en su función.\n",
    "\n",
    "- Documenta la función de manera adecuada (*docstrings*).\n",
    "\n",
    "- ¿Cómo se comporta, en términos del número de iterados, la convergencia del método de Newton?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Definición y documentación de la función*:\n",
    "\n",
    "Para implementar el método unidimensional, se puede hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    newton(f, fprime, x0, número_iteraciones)\n",
    "\n",
    "`newton` es una implementación unidimensional compleja del método de Newton para encontrar raíces de la función `f`.\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `f`, la función compleja (real) de variable compleja (real) de la que se quiere buscar una raíz (un punto \\$ a \\$ tales que \\$ f(a) = 0 \\$),\n",
    "* `f_prime`, la función derivada de `f`, y,\n",
    "* `x0`, una adivinanza inicial sobre la posición de la raíz.\n",
    "\n",
    "Opcionalmente, se puede especificar lo siguiente:\n",
    "\n",
    "* `número_iteraciones`, el número de iteraciones a realizar. (Por defecto está configurado en 1000 iteraciones.)\n",
    "\n",
    "`newton` requiere que tanto `f` como `fprime` sean funciones, que `x0` sea un número y que `número_iteraciones` sea un número entero. `x0`, en particular, siempre es convertida a un número flotante para mejorar la estabilidad de tipo.\n",
    "\n",
    "# Ejemplos\n",
    "```julia-repl\n",
    "    julia> newton(x -> x^2 - 2, x -> 2*x, 2, 5)\n",
    "    1.4142135623730951\n",
    "\n",
    "    julia> newton(x -> x^2 - 2, x -> 2*x, 1.4, 100)\n",
    "    1.414213562373095\n",
    "\n",
    "    julia> newton(x -> x^2 - 2, x -> 2*x, φ, 1)\n",
    "    1.4270509831248424\n",
    "\n",
    "    julia> newton(x -> x^2 - 2, x -> 2*x, φ, 10)\n",
    "    1.414213562373095\n",
    "\n",
    "    julia> newton(z -> z^2 + 1, z -> 2*z, -0.1*im, 5)\n",
    "    0.0 - 1.0032578510960606im\n",
    "\n",
    "    julia> newton(z -> z^2 + 1, z -> 2*z, -0.1*im, 10)\n",
    "    0.0 - 1.0im\n",
    "```\n",
    "\"\"\"\n",
    "function newton(f::Function, fprime::Function, x0::Number, número_iteraciones::Int = 1000)\n",
    "    \n",
    "    punto_actual = float(x0)\n",
    "    \n",
    "    for i in 1:número_iteraciones\n",
    "        punto_actual -= f(punto_actual)/fprime(punto_actual)\n",
    "    end\n",
    "    \n",
    "    return(punto_actual)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Ejemplo del cálculo de √2*:\n",
    "\n",
    "Considerando las funciones $f(x) = x^ 2 - a$ y $g(x) = f'(x) = 2x$ se tiene que si $a \\geq 0$, el método de Newton puede encontrar aproximaciones a las raíces cuadradas de $a$ según la adivinanza inicial otorgada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generando el ejemplo con los siguientes parámetros:\n",
    "\n",
    "f(x) = x^2 - 2\n",
    "g(x) = 2*x\n",
    "x0 = 1.5\n",
    "iteraciones = 10\n",
    "\n",
    "#La aproximación a la raíz cuadrada positiva de 2 calculada con el método es:\n",
    "\n",
    "@show raíz_newton = newton(f, g, x0, iteraciones)\n",
    "\n",
    "#Que el número calculado sea una aproximación a la raíz positiva es resultado de la elección de x0.\n",
    "#La aproximación a la raíz cuadrada (positiva) de 2 calculada con la función `sqrt` es:\n",
    "\n",
    "@show raíz = sqrt(2)\n",
    "\n",
    "#Comparando si son iguales como números flotantes de 64 bits:\n",
    "\n",
    "@show raíz_newton == raíz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que el método implementado calculó correctamente la raíz cuadrada positiva de 2 como número flotante de 64 bits con 10 iteraciones, con la adivinanza inicial igual a 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Análisis de la estabilidad de tipo*:\n",
    "Para ver la estabilidad de tipo en un ejemplo, basta con usar el macro `@code_warntype` en la llamada a la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el análisis se consideran las funciones y el número de iteraciones definidas para el ejemplo anterior. Lo que se va a modificar va a ser el tipo de adivinanza inicial suministrada.\n",
    "\n",
    "x0 = 1.5 #Lo mismo que en el ejemplo anterior. \n",
    "@show typeof(x0) #Número flotante de 64 bits.\n",
    "\n",
    "@code_warntype newton(f, g, x0, iteraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El macro no emitió advertencias. Intentando con un entero:\n",
    "\n",
    "x0 = 2\n",
    "@show typeof(x0) #Número entero de 64 bits.\n",
    "\n",
    "@code_warntype newton(f, g, x0, iteraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El macro tampoco emitió advertencias. Intentando con un irracional:\n",
    "\n",
    "x0 = φ #El número áureo\n",
    "@show typeof(x0) #Número irracional\n",
    "\n",
    "@code_warntype newton(f, g, x0, iteraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tampoco emitió advertencias, esto debido al comportamiento de un irracional ante la función float(): se toma la representación como número flotante de 64 bits más cercana a dicho irracional. \n",
    "\n",
    "float(φ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, intentando con un complejo de punto flotante:\n",
    "\n",
    "x0 = 2.0im\n",
    "@show typeof(x0) #Número complejo flotante de 64 bits\n",
    "\n",
    "@code_warntype newton(f, g, x0, iteraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tampoco arrojó advertencias. Estos ejemplos otorgan evidencia sobre la estabilidad de tipo de la función implementada. En todos los ejemplos se devolvió un número flotante de 64 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Análisis de la convergencia*:\n",
    "\n",
    "Para analizar la convergencia del método, consideremos el ejemplo en el que se calcula una aproximación a la raíz cuadrada de dos con una adivinanza inicial constante. ¿Cómo cambia la diferencia entre el valor dado por Julia y el valor calculado con el método de Newton según el número de iteraciones?\n",
    "\n",
    "Para hacer esto de forma automática y así poder graficar, conviene definir:\n",
    "\n",
    "* Una función que otorgue la diferencia entre el valor de la raíz cuadrada de un número $a$ calculada por el método de Newton implementado y el valor de la misma raíz cuadrada calculada por Julia con una adivinanza inicial fija y un número de iteraciones dado.\n",
    "    \n",
    "* Un conjunto de valores de esta diferencia para diferentes cantidades de iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    diferencia_raíz_cuadrada(a, x0, número_iteraciones)\n",
    "\n",
    "`diferencia_raíz cuadrada` calcula el valor absoluto de la diferencia entre el valor de la raíz cuadrada positiva del número real positivo \\$ a \\$ calculada mediante la función `sqrt` implementada en Julia y y la aproximación al valor de la misma raíz cuadrada calculada mediante el método de Newton con el número de iteraciones dado.\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `a`, el número real positivo del que se está calculando la raíz,\n",
    "* `x0`, una adivinaza de la raíz (positiva) de \\$ a \\$, y, \n",
    "* `número_iteraciones`, el número de iteraciones realizadas en el método de Newton para calcular la raíz.\n",
    "\n",
    "#Ejemplos:\n",
    "```julia-repl\n",
    "    julia> diferencia_raíz_cuadrada(2, 2, 1)\n",
    "    0.08578643762690485\n",
    "\n",
    "    julia> diferencia_raíz_cuadrada(2, 2, 3)\n",
    "    2.1239014147411694e-6\n",
    "\n",
    "    julia> diferencia_raíz_cuadrada(π, 1.5, 1)\n",
    "    0.02474370029108175\n",
    "\n",
    "    julia> diferencia_raíz_cuadrada(π, 1.5, 3)\n",
    "    8.183899780078718e-9\n",
    "```\n",
    "\"\"\"\n",
    "function diferencia_raíz_cuadrada(a::Real, x0::Real, número_iteraciones::Int)\n",
    "    \n",
    "    raíz_cuadrada_newton = newton(x -> x^2 - a, x -> 2*x, x0, número_iteraciones)\n",
    "    raíz_julia = sqrt(a)\n",
    "    \n",
    "    diferencia = raíz_julia - raíz_cuadrada_newton\n",
    "    valor_absoluto = abs(diferencia)\n",
    "    \n",
    "    return(valor_absoluto)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El conjunto de números de iteraciones para el cálculo muestra será de la forma: \n",
    "\n",
    "conjunto_iteraciones = 1:10\n",
    "\n",
    "#Generando el conjunto de valores para analizar lo que sucede. Se ocupa:\n",
    "\n",
    "x0 = 100\n",
    "\n",
    "#Esto debido a que se quiere evitar que la diferencia sea cero para poder usar una gráfica semilogarítimica para visualizar los datos:\n",
    "\n",
    "valores_raíz_newton = [diferencia_raíz_cuadrada(2, x0, i) for i in conjunto_iteraciones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se requiere cargar la siguiente paquetería:\n",
    "\n",
    "using Plots\n",
    "\n",
    "scatter(valores_raíz_newton, title = \"Convergencia del método a sqrt(2)\", label = \"x0 = $x0\", yscale = :log10, xlabel = \"Número de iteraciones\", ylabel = \"Diferencia entre valores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en la gráfica, al inicio de este ejemplo la convergencia del método es exponencial, y esta se acelera conforme aumenta el número de pasos. Esto se deduce de lo siguiente: sea $ y = A\\exp(kx)$ con $ A > 0 $, entonces, $ \\ln y = k x + \\ln A$. Esta última ecuación significa que una tendencia exponencial en una gráfica semilogarítmica con la escala logarítmica en la ordenada se visualiza como una línea recta. El cambio de comportamiento al final del ejemplo corresponde a un cambio en la velocidad de convergencia del método, en particular, dado que el cambio en el valor es mayor por paso, la convergencia se acelera. La convergencia total es más rápida que la convergencia exponencial definida por los primeros pasos del método.\n",
    "\n",
    "Esto, debe señalarse, debe de ser tomado como un ejemplo y no una demostración formal de la tasa de convergencia del método, en específico, porque la forma en la que se estudia la convergencia está limitada por el tipo de representación de los números usados en memoria y porque no se toman números arbitrarios para el estudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "- Usando la función que hicieron en el ejercicio anterior y variando la condición inicial `x0`, de -3 a 3 con pasos suficientemente pequeños, por ejemplo `0.125`, grafiquen la dependencia de la raíz encontrada de la condición inicial para $f(x)=x^2-2$. Para hacer esto, vale la pena que guarden en un vector la raíz obtenida y en otro la condición inicial.\n",
    "\n",
    "- Repitan el inciso anterior para $g(x) = (x-1)(x-2)(x-3)$, considerando el intervalo $x_0\\in[1,3]$ y muchas condiciones iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    comparación_valores(f, fprime, valor_inicial, valor_final, paso_valores, iteraciones_iniciales, iteraciones_finales, paso_iteraciones; legend)\n",
    "\n",
    "`comparación_valores` es una función que muestra en una misma gráfica el valor de las aproximaciones obtenidas para las raíces de `f` mediante el método de Newton en la muestra de puntos iniciales especificada en las iteraciones especificadas. Para modificar la leyenda de la gráfica se utilizan los mismos argumentos que en la función `plot`; por defecto, la leyenda se muestra en la esquina superior izquierda.\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `f`, la función real de variable real de la que se quiere buscar una raíz,\n",
    "* `f_prime`, la función derivada de `f`,\n",
    "* `valor_inicial`, la cota inferior del intervalo del cual se toma \\$ x_0 \\$,\n",
    "* `valor_final`, la cota superior del intervalo del cual se toma \\$ x_0 \\$,\n",
    "* `paso_valores`, la distancia entre las muestras de \\$ x_0 \\$ a considerar,\n",
    "* `iteraciones_iniciales`, el número de iteraciones mínimo a usar,\n",
    "* `iteraciones_finales`, el número de iteraciones máximo a usar, y,\n",
    "* `paso_iteraciones`, la distancia entre las iteraciones a considerar.\n",
    "\n",
    "Un argumento optativo es:\n",
    "\n",
    "* `legend`, que ocupa los mismos *keyword arguments* de la función `plot` para modificar la posición de la leyenda en la gŕafica.\n",
    "\n",
    "`comparación_valores` requiere que tanto `f` como `fprime` sean funciones; que `valor_inicial`, `valor_final` y `paso_valores`  sean reales; y que `iteraciones_iniciales`, `iteraciones_finales` y `paso_iteraciones` sean un números enteros. `valor_inicial`, `valor_final` y `paso_valores` en particular son convertidos en números flotantes de 64 bits.\n",
    "\n",
    "# Ejemplo:\n",
    "\n",
    "Si se desea visualizar las aproximaciones al valor de las raíces de la función `x -> x^2 - 2`  dadas por el método de Newton en el intervalo \\$ [0,1]  \\$ con paso de 0.01 con 3, 5 y 7 iteraciones se puede usar el comando:\n",
    "```julia-repl\n",
    "    julia> comparación_valores(x -> x^2 - 2, x -> 2*x, 0, 1, 0.01, 3, 7, 2)\n",
    "```\n",
    "\n",
    "# Notas:\n",
    "\n",
    "**Nota**: Esta función, aunque sirve, no es computacionalmente eficiente ya que todas las iteraciones anteriores se vuelven a calcular cada vez que se calcula la siguiente. Esto es por la definición de la función `newton`; una sugerencia para evitar esto sería definir una función `iterados_newton` que guarde los resultados del cálculo a cada paso en un arreglo y que devuelva este. Asimismo, para implementar esto, probablemente se tenga que reestructurar la forma en la que están anidados los bucles `for`.\n",
    "\"\"\"\n",
    "function comparación_valores(f::Function, fprime::Function, valor_inicial::Real, valor_final::Real, paso_valores::Real, iteraciones_iniciales::Int, iteraciones_finales::Int, paso_iteraciones::Int; legend = :topleft)\n",
    "    \n",
    "    valor_inicial_flotante = float(valor_inicial)\n",
    "    valor_final_flotante = float(valor_final)\n",
    "    paso_valores_flotante = float(paso_valores)\n",
    "    puntos_muestra = valor_inicial_flotante:paso_valores:valor_final_flotante\n",
    "    gráfica = plot()\n",
    "\n",
    "    for i in iteraciones_iniciales:paso_iteraciones:iteraciones_finales\n",
    "        \n",
    "        valores_raíz_iteraciones_i = []\n",
    "        \n",
    "        for j in puntos_muestra\n",
    "            \n",
    "            raíz = newton(f, fprime, j, i)\n",
    "    \n",
    "            push!(valores_raíz_iteraciones_i, raíz)\n",
    "        end\n",
    "        \n",
    "        plot!(gráfica, puntos_muestra, valores_raíz_iteraciones_i, title = \"Comparación del valor de la raíz según la posición inicial\", xlabel = \"x0\", ylabel = \"Raíz calculada\", label = \"iteraciones = $i\", legend = legend)\n",
    "    end\n",
    "\n",
    "    return(gráfica)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de las raíces de  $f(x) = x^2 - 2$.\n",
    "\n",
    "Para realizar el estudio para $f(x) = x^2 - 2$, (re)definamos las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = x^2 - 2\n",
    "fprime(x) = 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo la comparación solicitada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_inicial = -3\n",
    "valor_final = 3\n",
    "paso_valores = 0.01\n",
    "iteraciones_iniciales = 1\n",
    "iteraciones_finales = 10\n",
    "paso_iteraciones = 3\n",
    "\n",
    "comparación_raíz_1 = comparación_valores(f, fprime, valor_inicial, valor_final, paso_valores, iteraciones_iniciales, iteraciones_finales, paso_iteraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, conviene redimensionar el área que se muestra en la gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot!(comparación_raíz_1, ylim = (-5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, a mayor número de iteraciones, el método mejora su convergencia a los valores conocidos de las raíces a lo largo de los diferentes valores de $x_0$ mostrados. Los \"picos\" observados en torno a $x_0 = 0$ corresponden al cambio de la raíz que arroja el método (de la raíz negativa a la raíz positiva). La forma de estos picos es explicada por el hecho de que $x_0 = 0$ es un punto crítico de $f$, lo que indetermina el cociente al dar un paso con el método. Obsérvese que la velocidad de convergencia en torno a $x_0 \\approx \\pm \\sqrt 2$ es máxima (las curvas generadas de considerar diferentes números de iteraciones son más cercanas en estas regiones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de las raíces de  $g(x) = (x-1)(x-2)(x-3)$.\n",
    "Para realizar el estudio para $g(x) = (x-1)(x-2)(x-3)$, (re)definamos las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = (x-1)*(x-2)*(x-3)\n",
    "gprime(x) = (x-2)*(x-3) + (x-1)*(x-3) + (x-1)*(x-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo la comparación solicitada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_inicial = 0\n",
    "valor_final = 3\n",
    "paso_valores = 0.001\n",
    "iteraciones_iniciales = 10\n",
    "iteraciones_finales = 100\n",
    "paso_iteraciones = 10\n",
    "\n",
    "comparación_raíz_2 = comparación_valores(g, gprime, valor_inicial, valor_final, paso_valores, iteraciones_iniciales, iteraciones_finales, paso_iteraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de la función $ g(x) $ se observa, salvo en el caso en el que se tomaron 10 iteraciones en cada punto, que existe una convergencia rápida a valores específicos dentro de la \"malla\" de puntos iniciales; sin embargo, existen ciertas regiones, aparentemente puntuales, en las que hay un comportamiento que no se ajusta al comportamiento general de la gráfica (valores constantes de la raíz calculada en intervalos \"más o menos\" grandes). \n",
    "\n",
    "Haciendo una gráfica más detallada de sólo los resultados producto de 100 iteraciones de cada punto, se puede visualizar de forma más sencilla este hecho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_inicial = 0\n",
    "valor_final = 3\n",
    "paso_valores = 0.001\n",
    "iteraciones_iniciales = 100\n",
    "iteraciones_finales = 100\n",
    "paso_iteraciones = 1\n",
    "\n",
    "comparación_raíz_2_detalle = comparación_valores(g, gprime, valor_inicial, valor_final, paso_valores, iteraciones_iniciales, iteraciones_finales, paso_iteraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, de manera burda, existen cinco intervalos visibles del conjunto de puntos muestra en la gráfica anterior en los que cambia el valor de la raíz calculada. En todos los casos, salvo en los que existen cambios en el valor predicho de la raíz, se obtienen los valores esperados de las raíces de la función $ g(x) $ ($ x_0  = 1, 2, 3$). Existen cuatro puntos en torno a $ x_0 = 1.5 $ y $ x_0 = 2.4 $ que generan un comportamiento que aparenta ser muy localizado. Realizando un estudio detallado de estos puntos, como el mostrado a continuación, se notará que en realidad este comportamiento no es puntual como se puede pensar en primera aproximación, sino que son efectos de la malla de puntos muestra usada para generar la gráfica. Esto quiere decir que, en principio, son nueve los intervalos \"visibles\" con esta malla de puntos iniciales en los que se aprecia un cambio de resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_inicial = 1.5\n",
    "valor_final = 1.6\n",
    "paso_valores = 0.001\n",
    "iteraciones_iniciales = 100\n",
    "iteraciones_finales = 100\n",
    "paso_iteraciones = 1\n",
    "\n",
    "comparación_raíz_2_detalle_puntos_1 = comparación_valores(g, gprime, valor_inicial, valor_final, paso_valores, iteraciones_iniciales, iteraciones_finales, paso_iteraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la gráfica anterior, resulta evidente que cerca de $x_0 = 1.530$ y de $x_0 = 1.550$ en efecto existen intervalos (\"pequeños\") de puntos que arrojan un resultado distinto al que se espera de los intervalos que les rodean, pero que entran dentro de los valores esperados para las raíces. La inclinación de las líneas en torno de estos puntos no es resultado de predicciones de raíces inesperadas de forma analítica, sino un efecto de la finitud de los puntos usados para graficar. (Una nota interesante: si se mejora la resolución de la malla en aproximadamente diez veces, en esta gráfica se encuentra ¡otro \"pico\" de valores que no se observa en la actual! Esto de contar intervalos en los que se aprecia o no una uniformidad de comportamiento es dependiente de la \"fineza\" de la malla.)\n",
    "\n",
    "Para el otro punto en el que se aprecia un comportamiento similar ($x_0 = 2.4$): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_inicial = 2.4\n",
    "valor_final = 2.5\n",
    "paso_valores = 0.001\n",
    "iteraciones_iniciales = 100\n",
    "iteraciones_finales = 100\n",
    "paso_iteraciones = 1\n",
    "\n",
    "comparación_raíz_2_detalle_puntos_2 = comparación_valores(g, gprime, valor_inicial, valor_final, paso_valores, iteraciones_iniciales, iteraciones_finales, paso_iteraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los comentarios realizados para el primer detalle en torno a $ x_0 = 1.5 $ se mantienen para esta gráfica, salvo la posición de los valores en los que cambia el comportamiento de la misma: $x_0 = 2.445$ y $ x_0 = 2.465 $. Esto también incluye lo mencionado sobre el \"pico\" no visible con esta resolución. Otro comentario importante es que esta gráfica se comporta como la reflexión especular respecto al eje $ y $ de la reflexión especular en torno al eje $ x $ de la gráfica anterior.\n",
    " \n",
    "Como punto final, y para tratar de entender la existencia de estos picos tan \"pequeños\", conviene graficar la función y sus primeras dos derivadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo la segunda derivada:\n",
    "gdoubleprime(x) = 6*x - 12\n",
    "\n",
    "#Procediendo a graficar:\n",
    "valor_inicial = 0\n",
    "valor_final = 3\n",
    "paso_valores = 0.001\n",
    "dominio = valor_inicial:paso_valores:valor_final\n",
    "\n",
    "gráfica_g = plot(dominio, g, label = \"g(x)\")\n",
    "plot!(gráfica_g, dominio, gprime, label = \"g'(x)\")\n",
    "plot!(gráfica_g, dominio, gdoubleprime, label = \"g''(x)\", xlabel = \"x\", ylabel = \"y\", title = \"Gráfica de la función g(x) y su primer y segunda derivada\", legend = :topleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la gráfica de la función, las tres raíces analíticas esperadas en efecto son raíces de la función. Más aún, la derivada se anula en torno a los puntos en los que se detecta un comportamiento \"puntual\" de la función: esto permite explicar el por qué este comportamiento \"errático\" ocurre en la vecindad de estos puntos. Finalmente, el hecho de la \"doble reflexión especular\" en torno a estos puntos con \"comportamiento errático\" se puede explicar debido al cambio de signo de la segunda derivada en el sitio en el que se situaría el origen de los espejos (en torno a $ x_0 = 1.8$, visible en la primer gráfica de las raíces de la función $g(x)$), lo que responde a un cambio en la curvatura de la función $ g(x)$: en lugar de que la derivada disminuya conforme se avanza en $ x $, como ocurre antes del punto de inflexión comentado, la derivada aumenta conforme se avanza en $x$, lo que cambia la forma en la que las raíces van apareciendo en la gráfica inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Considerando la función $h(z)=z^3-c$, con $z\\in\\mathbb{C}$ y $c=1$, utiliza el método de Newton para encontrar las raices en el plano complejo de dicha ecuación. Repite el ejercicio de la dependencia de la raíz encontrada de las condiciones iniciales, graficando en el plano de Argand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiendo la función $h(z) = z^3 - 1 $ y su derivada: $h'(z) = 2z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "h(z) = z^3 - 1;\n",
    "hprime(z) = 3*z^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se sabe que las raíces de la ecuación $ h(z) = 0 $ son: $ z= e^{\\frac{2 n \\pi i}{3}} $ con $ n \\in \\{0, 1, 2\\}$. En principio, se pueden proponer las siguientes tres condiciones iniciales de manera \"educada\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = 2 + 0im\n",
    "z1 = 2im\n",
    "z2 = -2im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas condiciones corresponden a puntos sobre los ejes a una distancia de 2 unidades del origen, elegidos de tal forma que cada uno de ellos estén más cerca de una raíz distinta. Calculando las raíces con el método de Newton del ejercicio 1 (válido también para funciones complejas) y comparando con la raíz analítica esperada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para z0, con raíz analítica esperada 1:\n",
    "\n",
    "iteraciones = 10\n",
    "@show raíz_z0_numérica = newton(h, hprime, z0, iteraciones)\n",
    "@show raíz_z0_analítica = exp((2*0*π*im)/3)\n",
    "@show raíz_z0_numérica == raíz_z0_analítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para z1, con raíz analítica esperada exp(2*π*i/3):\n",
    "\n",
    "iteraciones = 10\n",
    "@show raíz_z1_numérica = newton(h, hprime, z1, iteraciones)\n",
    "@show raíz_z1_analítica = exp((2*1*π*im)/3)\n",
    "@show raíz_z1_numérica == raíz_z1_analítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para z2, con raíz analítica esperada exp(2*π*i/3):\n",
    "\n",
    "iteraciones = 10\n",
    "@show raíz_z2_numérica = newton(h, hprime, z2, iteraciones)\n",
    "@show raíz_z2_analítica = exp((2*2*π*im)/3)\n",
    "@show raíz_z2_numérica == raíz_z2_analítica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que los valores obtenidos numéricamente aparentemente no concuerdan entre si en el caso de las raíces distintas de uno. El error resulta relativamente pequeño ya que es, cuanto más, el doble del *épsilon de la máquina* (la diferencia mínima entre la representación del número uno y el siguiente flotante al número uno).\n",
    "\n",
    "El épsilon de la máquina para flotantes de 64 bits es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "épsilon_64 = eps(Float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia entre el valor analítico esperado y el valor numérico obtenido para `z1` es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "δ1 = raíz_z1_analítica - raíz_z1_numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando los valores absolutos de las partes real e imaginaria de la diferencia con el épsilon de la máquina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show abs(real(δ1)) == épsilon_64 #Igual al épsilon\n",
    "@show abs(imag(δ1)) < épsilon_64 #Menor que el épsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repitiendo el análisis para `z2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La diferencia entre el valor analítico esperado y el valor obtenido para z2 es:\n",
    "\n",
    "@show δ2 = raíz_z2_analítica - raíz_z2_numérica\n",
    "\n",
    "#Comparando los valores absolutos de las partes real e imaginaria de la diferencia con el épsilon de la máquina:\n",
    "\n",
    "@show abs(real(δ2)) == 2*épsilon_64 #Igual a dos veces el épsilon de la máquina.\n",
    "@show abs(imag(δ2)) == épsilon_64 #Igual al épsilon de la máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, aunque existen errores, estos son relativamente pequeños. La comparación con el épsilon de la máquina resulta adecuada dado que las raíces tienen norma unitaria, aunque es necesario mencionar que la diferencia entre un flotante y el siguiente, al menos en Julia, depende del flotante mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La función nextfloat otorga el siguiente flotante al flotante dado.\n",
    "#Si a = 0.0, la diferencia con el siguiente flotante a cero es:\n",
    "\n",
    "a = 0.0\n",
    "@show nextfloat(a) - a\n",
    "\n",
    "#Si a = 1.0, la diferencia con el siguiente flotante a uno (el épsilon de la máquina) es:\n",
    "a = 1.0\n",
    "@show nextfloat(a) - a\n",
    "\n",
    "#Si a = 1e300, la diferencia con el siguiente flotante a 1e300 es:\n",
    "a = 1e300\n",
    "@show nextfloat(a) - a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificando el número de iteraciones en el cálculo de las raíces se observa que los valores se estancan en los mostrados anteriormente. Lo que podría estar sucediendo es que en el momento de querer dar más pasos, las diferencias generadas por el método son menores a la distancia entre flotantes distintos que puede reconocer la máquina. Una forma de hacer esto es aumentar la precisión de los números con los que se trabaja. Para poder analizar esto, se sugiere hacer el estudio separando componentes real e imaginaria y utilizar flotantes de precisión arbitraria (`BigFloat`). (No pude encontrar referencias que hablaran de complejos de precisión arbitraria, probablemente porque los complejos de un \"tipo de precisión\" pueden tratarse como duplas de flotantes de \"medio tipo de precisión\", como lo sugiere la documentación de `Complex`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráficas de la convergencia del método a las raíces con diferentes posiciones iniciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviene definir una función que grafique la trayectoria del método a lo largo de un conjunto de iteraciones de un conjunto de condiciones iniciales dado. Para facilitar la visualización, se utilizan esquemas de colores predefinidos en la paquetería `Colors.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargando la paquetería:\n",
    "using Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo la función:\n",
    "\n",
    "\"\"\"\n",
    "    comparación_valores_complejos(f, fprime, valores_iniciales, número_iteraciones; colorscheme)\n",
    "\n",
    "`comparación_valores_complejos` es una función que muestra en una misma gráfica del plano complejo el valor de los datos obtenidos de la implementación del método de Newton sobre una muestra de puntos iniciales especificada a lo largo del número de iteraciones dado para la función `f`. Cada trayectoria es graficada con un color distinto y el color del marcador de una iteración particular es el mismo en todas las trayectorias. El color de los marcadores está dado por la función `colorscheme` de la paquetería `Colors.jl`. El valor por defecto del esquema de color es `\"Blues\"`. Esto quiere decir que, por defecto, el color del marcador para una iteración dada va de azul claro a azul oscuro conforme el valor de la iteración correspondiente aumenta.\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `f`, la función compleja de variable compleja de la que se quiere buscar una raíz,\n",
    "* `f_prime`, la función derivada de `f`,\n",
    "* `valores`, el arreglo unidimensional de números que se usarán como valores iniciales para el método de Newton, y,\n",
    "* `número_iteraciones`, el número de iteraciones del método de Newton mostradas en la gráfica.\n",
    "\n",
    "Un argumento optativo es:\n",
    "\n",
    "* `colorscheme`, que determina el esquema de color a usar para los marcadores. Los esquemas de color definidos por la paquetería `Colors.jl` pueden ser usados para alterar el color de los marcadores de la siguiente manera:\n",
    "\n",
    "```julia-repl\n",
    "    colorscheme = \"Reds\"\n",
    "```\n",
    "\n",
    "`comparación_valores_complejos` requiere que tanto `f` como `fprime` sean funciones, que `valores_iniciales`  sea un arreglo unidimensional de números, y que `número de iteraciones` sea un entero. Los valores iniciales suministrados, en particular, son convertidos en números complejos flotantes de 64 bits.\n",
    "\n",
    "# Ejemplo:\n",
    "\n",
    "Si se desea visualizar las aproximaciones al valor de las raíces de la función `z -> z^3 - 1`  dadas por el método de Newton con condiciones inciales dadas por \\$ 2, 2i \\$ y \\$ - 2i \\$ a lo largo de 10 iteraciones se puede usar el comando:\n",
    "```julia-repl\n",
    "    julia> comparación_valores_complejos(z -> z^3 - 1, z -> 2*z, [2, 2im, -2im], 10)\n",
    "```\n",
    "\n",
    "# Notas:\n",
    "\n",
    "**Nota**: Esta función, aunque sirve, no es computacionalmente eficiente ya que todas las iteraciones anteriores se vuelven a calcular cada vez que se calcula la siguiente. Esto es por la definición de la función `newton`; una sugerencia para evitar esto sería definir una función `iterados_newton` que guarde los resultados del cálculo a cada paso en un arreglo y que devuelva este. Asimismo, para implementar esto, probablemente se tenga que reestructurar la forma en la que están anidados los bucles `for`.\n",
    "\"\"\"\n",
    "function comparación_valores_complejos(f::Function, fprime::Function, valores_iniciales::Array, número_iteraciones::Int; colorscheme = \"Blues\")\n",
    "    \n",
    "    gráfica = plot()\n",
    "    colores = colormap(colorscheme, número_iteraciones + 1) #Facilita los colores\n",
    "    \n",
    "    for i in valores_iniciales\n",
    "    \n",
    "        valor = Complex{Float64}(i) #Convierte la entrada en un complejo con entradas de punto flotante de 64 bits\n",
    "        puntos = [valor]\n",
    "\n",
    "        for j in 1:número_iteraciones\n",
    "    \n",
    "            valor = newton(f, fprime, valor, j)\n",
    "            push!(puntos, valor)\n",
    "        end\n",
    "        \n",
    "        plot!(gráfica, puntos, label = \"$i\")\n",
    "        scatter!(gráfica, puntos, color = colores, label = \"\", title = \"Algunas trayectorias generadas con el método de Newton\")\n",
    "    end\n",
    "    \n",
    "    return(gráfica)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder estudiar la dependencia de las raíces encontradas respecto de la condición inicial, se harán tres visualizaciones de diez condiciones iniciales con argumentos de la forma $ \\theta = \\frac{2 k \\pi}{10}$ con $ k \\in \\{0, 1, 2, \\ldots, 9\\}$. La diferencia entre visualizaciones estará dada por la distancia de los puntos al origen: 0.5, 1 y 2 unidades respectivamente.\n",
    "\n",
    "Para simplificar el trabajo, conviene definir lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generando el espacio de argumentos:\n",
    "\n",
    "n = 10 \n",
    "ángulos = [2*k*π/n for k in 0:(n-1)]\n",
    "\n",
    "#Conviene generar el espacio de vectores unitarios con argumentos iguales a los ángulos anteriores:\n",
    "\n",
    "unitarios = exp.(im*ángulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la primer visualización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generando la visualización con r1 = 0.5 y 6 iteraciones:\n",
    "\n",
    "r1 = 0.5\n",
    "iteraciones = 6\n",
    "condiciones_iniciales_1 = r1*unitarios\n",
    "visualización_1 = comparación_valores_complejos(h, hprime, condiciones_iniciales_1, iteraciones)\n",
    "\n",
    "#Para mejorar la legibilidad, se grafica un círculo con radio r1:\n",
    "\n",
    "muestra_círculo = 0:0.01:2π\n",
    "cosenos = cos.(muestra_círculo)\n",
    "senos = sin.(muestra_círculo)\n",
    "\n",
    "plot!(visualización_1, r1*cosenos, r1*senos, key = false, aspectratio = :equal, title = \"Convergencia de condiciones iniciales con r = $r1\", xlims = (-2, 2), ylims = (-2, 2))\n",
    "\n",
    "#Y finalmente, se grafican las raíces de la función en rojo:\n",
    "\n",
    "scatter!(visualización_1, [raíz_z0_analítica, raíz_z1_analítica, raíz_z2_analítica], color = :red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que, contrario a lo que se esperaría, en este caso las trayectorias generadas por los puntos no convergen siempre a la raíz más cercana. Las raíces con argumento $\\theta = \\{ 0, \\pi\\}$ convergen a $z = 1$; aquellas con argumento $\\theta = \\{ \\frac{2\\pi}{10}, \\frac{6\\pi}{10}, \\frac{8\\pi}{10}, \\frac{16\\pi}{10} \\}$ convergen a $ z = \\exp(\\frac{2\\pi i}{3})$; y finalmente, aquellas con argumento $\\theta = \\{ \\frac{4\\pi}{10}, \\frac{12\\pi}{10}, \\frac{14\\pi}{10}, \\frac{18\\pi}{10}\\}$ convergen a $ z = \\exp(\\frac{4\\pi i}{3})$.\n",
    "\n",
    "Para la segunda visualización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generando la visualización con r2 = 1 y 6 iteraciones:\n",
    "\n",
    "r2 = 1\n",
    "iteraciones = 6\n",
    "condiciones_iniciales_2 = r2*unitarios\n",
    "visualización_2 = comparación_valores_complejos(h, hprime, condiciones_iniciales_2, iteraciones)\n",
    "\n",
    "#Para mejorar la legibilidad, se grafica un círculo con radio r2:\n",
    "\n",
    "plot!(visualización_2, r2*cosenos, r2*senos, key = false, aspectratio = :equal, title = \"Convergencia de condiciones iniciales con r = $r2\", xlims = (-2, 2), ylims = (-2, 2))\n",
    "\n",
    "#Y finalmente, se grafican las raíces de la función en rojo:\n",
    "\n",
    "scatter!(visualización_2, [raíz_z0_analítica, raíz_z1_analítica, raíz_z2_analítica], color = :red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que las trayectorias generadas por los puntos, salvo por $ z = \\exp(i \\pi)$, convergen siempre a la raíz más cercana. Las raíces con argumento $\\theta = \\{ 0, \\frac{2\\pi}{10}, \\pi, \\frac{18\\pi}{10}\\}$ convergen a $z = 1$; aquellas con argumento $\\theta = \\{ \\frac{4\\pi}{10}, \\frac{6\\pi}{10}, \\frac{8\\pi}{10}\\}$ convergen a $ z = \\exp(\\frac{2\\pi i}{3})$; y finalmente, aquellas con argumento $\\theta = \\{ \\frac{12\\pi}{10}, \\frac{14\\pi}{10}, \\frac{16\\pi}{10}\\}$ convergen a $ z = \\exp(\\frac{4\\pi i}{3})$.\n",
    "\n",
    "Para la tercer visualización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Generando la visualización con r3 = 2 y 6 iteraciones:\n",
    "\n",
    "r3 = 2\n",
    "iteraciones = 6\n",
    "condiciones_iniciales_3 = r3*unitarios\n",
    "visualización_3 = comparación_valores_complejos(h, hprime, condiciones_iniciales_3, iteraciones)\n",
    "\n",
    "#Para mejorar la legibilidad, se grafica un círculo con radio r3:\n",
    "\n",
    "plot!(visualización_3, r3*cosenos, r3*senos, key = false, aspectratio = :equal, title = \"Convergencia de condiciones iniciales con r = $r3\", xlims = (-3, 3), ylims = (-3, 3))\n",
    "\n",
    "#Y finalmente, se grafican las raíces de la función en rojo:\n",
    "\n",
    "scatter!(visualización_3, [raíz_z0_analítica, raíz_z1_analítica, raíz_z2_analítica], color = :red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que las trayectorias generadas por los puntos, salvo por $ z = \\exp(i \\pi)$, convergen siempre a la raíz más cercana. Las raíces con argumento $\\theta = \\{ 0, \\frac{2\\pi}{10}, \\pi, \\frac{18\\pi}{10}\\}$ convergen a $z = 1$; aquellas con argumento $\\theta = \\{ \\frac{4\\pi}{10}, \\frac{6\\pi}{10}, \\frac{8\\pi}{10}\\}$ convergen a $ z = \\exp(\\frac{2\\pi i}{3})$; y finalmente, aquellas con argumento $\\theta = \\{ \\frac{12\\pi}{10}, \\frac{14\\pi}{10}, \\frac{16\\pi}{10}\\}$ convergen a $ z = \\exp(\\frac{4\\pi i}{3})$.\n",
    "\n",
    "Como se puede observar en todos los ejemplos usados, los valores suministrados siempre convergen a los valores analíticos de las raíces de la función $h(z) = z^3 - 1$; sin embargo, la convergencia a los valores no siempre es hacia la raíz más cercana al punto usado como condición inicial. Estos ejemplos no determinan la convergencia del método en todo el plano complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "- Definan una función `derivada_derecha` que calcule *numéricamente* la derivada de una función $f(x)$ de una variable (a priori arbitaria), en un punto $x_0$. Para esto, utilizaremos la aproximación de la derivada\n",
    "que se basa en su definición:\n",
    "\n",
    "$$ \n",
    "f'(x_0) \\approx \\frac{\\Delta f_+}{\\Delta x} \\equiv \\frac{f(x_0+h)-f(x_0)}{h},\n",
    "$$\n",
    "\n",
    "lo que requiere de la especificación de `h`. (Este método también se conoce por el nombre de *diferencias finitas*.)\n",
    "\n",
    "- A fin de simular el $\\lim_{h\\to 0}$, consideren distintos valores de $h$ que precisamente simulen dicho límite. Para cada valor de $h$ calculen el error absoluto del cálculo numérico respecto al valor *exacto*. Ilustren esto en una gráfica del error vs $h$, para $f(x) = 3x^3-2$, en $x_0=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "**Definición de la derivada derecha**:\n",
    "\n",
    "Se define la función que calcula la derivada numérica derecha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    derivada_derecha(f, x0, h)\n",
    "\n",
    "`derivada_derecha` es una función que calcula la derivada numérica derecha de la función dada en el punto suministrado con el tamaño de paso definido.\n",
    "\n",
    "# Definición\n",
    "\n",
    "La derivada numérica derecha de `f` en el punto `x0` con tamaño de paso `h` está dada por: \n",
    "\n",
    "\\$ f'_+ (x_0; h) = \\frac{f(x_0 + h) - f(x_0)}{h}. \\$\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `f`, la función (compleja) de variable (compleja) de la que se quiere calcular la derivada,\n",
    "* `x0`, el punto en el que se desea calcular la derivada, y,\n",
    "* `h`, el tamaño de paso usado para aproximar la derivada exacta.\n",
    "\n",
    "`derivada_derecha` requiere que `f` sea una función, y que `x0` y `h` sean números. En particular, para mejorar la estabilidad de tipo, `x0` y `h` son convertidos a números flotantes de 64 bits.\n",
    "\n",
    "# Ejemplos:\n",
    "```julia-repl\n",
    "    julia> derivada_derecha(x -> x, 1, 0.01)\n",
    "    1.0000000000000009\n",
    "\n",
    "    julia> derivada_derecha(x -> x^2, 1, 0.01)\n",
    "    2.0100000000000007\n",
    "\n",
    "    julia> derivada_derecha(x -> sin(x), 0, 0.001)\n",
    "    0.9999998333333416\n",
    "\n",
    "    julia> derivada_derecha(x -> exp(x), 0, 0.001)\n",
    "    1.0005001667083846\n",
    "```\n",
    "\"\"\"\n",
    "function derivada_derecha(f::Function, x0::Number, h::Number)\n",
    "    \n",
    "    x0 = float(x0)\n",
    "    h = float(h)\n",
    "    \n",
    "    derivada = (f(x0 + h) - f(x0))/h\n",
    "    \n",
    "    return(derivada)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición de la función de error**:\n",
    "\n",
    "Se define la función que calcula el valor absoluto de la diferencia entre la derivada numérica solicitada y la derivada analítica en el mismo punto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    error_derivada(f, fprime, x0, h, derivada)\n",
    "\n",
    "`error_derivada` es una función que calcula, en un punto específico, el valor absoluto de la diferencia entre la derivada numérica obtenida con el método y tamaño de paso dados y la derivada analítica suministrada.\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `f`, la función (compleja) de variable (compleja) de la que se quiere calcular la derivada,\n",
    "* `fprime`, la función derivada de `f`,\n",
    "* `x0`, el punto en el que se desea calcular el error,\n",
    "* `h`, el tamaño de paso usado para aproximar la derivada exacta, y,\n",
    "* `derivada`, la función usada para calcular la derivada numérica con la siguiente sintaxis: `derivada(f, x0, h)`.\n",
    "\n",
    "`error_derivada` requiere que `f`, `fprime` y `derivada` sean funciones y que `x0` y `h` sean números. En particular, para mejorar la estabilidad de tipo, `x0` y `h` son convertidos a números flotantes de 64 bits.\n",
    "\n",
    "# Ejemplos:\n",
    "```julia-repl\n",
    "    julia> error_derivada(x -> x, x -> 1, 1, 0.01, derivada_derecha)\n",
    "    8.881784197001252e-16\n",
    "\n",
    "    julia> error_derivada(x -> sin(x), x -> cos(x), 1, 0.01, derivada_derecha)\n",
    "    0.00421632485627077\n",
    "\n",
    "    julia> error_derivada(x -> sin(x), x -> cos(x), 1, 0.01, derivada_simétrica)\n",
    "    9.004993406280803e-6\n",
    "\n",
    "    julia> error_derivada(x -> sin(x), x -> cos(x), 1, 0.01, derivada_compleja)\n",
    "    9.00508345647033e-6\n",
    "```\n",
    "\"\"\"\n",
    "function error_derivada(f::Function, fprime::Function, x0::Number, h::Number, derivada::Function)\n",
    "    \n",
    "    x0 = float(x0)\n",
    "    h = float(h)\n",
    "    \n",
    "    valor_analítico = fprime(x0)\n",
    "    valor_numérico = derivada(f, x0, h)\n",
    "    \n",
    "    diferencia_absoluta = abs(valor_numérico - valor_analítico)\n",
    "    \n",
    "    return(diferencia_absoluta)\n",
    "end    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estudio del error de la derivada numérica derecha de acuerdo al tamaño del paso**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo la función a usar de ejemplo:\n",
    "\n",
    "f₄(x) = 3*x^3 - 2\n",
    "f₄prime(x) = 9*x^2\n",
    "\n",
    "#Definiendo el punto a estudiar:\n",
    "\n",
    "x0 = 1\n",
    "\n",
    "#Generando una gráfica del error como función del tamaño del paso:\n",
    "\n",
    "log₂_recíproco_h_muestra = 0:60 #El arreglo de menos el logaritmo base 10 de los puntos muestra.\n",
    "h_muestra = [1/(2^i) for i in log₂_recíproco_h_muestra] #El arreglo de los puntos muestra, potencias recíprocas de 2.\n",
    "\n",
    "errores_f₄_derivada_derecha = []\n",
    "\n",
    "for h_paso in h_muestra\n",
    "    \n",
    "    error = error_derivada(f₄, f₄prime, x0, h_paso, derivada_derecha)\n",
    "    push!(errores_f₄_derivada_derecha, error)\n",
    "end\n",
    "\n",
    "gráfica_f₄_derivada_derecha = plot(h_muestra, errores_f₄_derivada_derecha, xscale = :log10, legend = false, xlabel = \"h\", ylabel = \"Valor absoluto del error\", title = \"Error de la derivada numérica derecha de f4 como función del tamaño del paso en x0 = $x0\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una mejora logarítmica, aunque \"rápida\" al inicio (cuando $h \\approx 1$), del error. Este tiene un valor menor a $10^{-6}$ cuando $ h \\lessapprox 10^{-8} $. Sin embargo, en torno a $h \\approx 5 \\times 10^{-16}$, el error aumenta de manera logarítmica para estabilizarse cerca de 10 cuando $h \\lessapprox 10^{-16}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "- Repitan el ejercicio anterior (escribe una función `derivada_simetrica`, usando ahora la aproximación *simétrica* (respecto a la ocurrencia de $h$) de la derivada, dada por\n",
    "\n",
    "$$\n",
    "f'(x_0) \\approx \\frac{ \\Delta f_{sym}}{\\Delta x} = \\lim_{h\\to 0} \\frac{f(x_0+h)-f(x_0-h)}{2h}.\n",
    "$$\n",
    "\n",
    "- ¿Por qué es correcto afirmar que la derivada simétrica resulta en una mejor aproximación que la derivada derecha? Argumenten y si es necesario usen argumentos analíticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición de la derivada simétrica**:\n",
    "\n",
    "Se define la función que calcula la derivada numérica simétrica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    derivada_simétrica(f, x0, h)\n",
    "\n",
    "`derivada_simétrica` es una función que calcula la derivada numérica simétrica de la función dada en el punto suministrado con el tamaño de paso definido.\n",
    "\n",
    "# Definición\n",
    "\n",
    "La derivada numérica simétrica de `f` en el punto `x0` con tamaño de paso `h` está dada por: \n",
    "\n",
    "\\$ f'_{sym} (x_0; h) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h}. \\$\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `f`, la función (compleja) de variable (compleja) de la que se quiere calcular la derivada,\n",
    "* `x0`, el punto en el que se desea calcular la derivada, y,\n",
    "* `h`, el tamaño de paso usado para aproximar la derivada exacta.\n",
    "\n",
    "`derivada_simétrica` requiere que `f` sea una función, y que `x0` y `h` sean números. En particular, para mejorar la estabilidad de tipo, `x0` y `h` son convertidos a números flotantes de 64 bits.\n",
    "\n",
    "# Ejemplos:\n",
    "```julia-repl\n",
    "    julia> derivada_simétrica(x -> x, 1, 0.01)\n",
    "    1.0000000000000009\n",
    "\n",
    "    julia> derivada_simétrica(x -> x^2, 1, 0.01)\n",
    "    2.0000000000000018\n",
    "\n",
    "    julia> derivada_simétrica(x -> sin(x), 0, 0.001)\n",
    "    0.9999998333333416\n",
    "\n",
    "    julia> derivada_simétrica(x -> exp(x), 0, 0.001)\n",
    "    1.0000001666666813\n",
    "```\n",
    "\"\"\"\n",
    "function derivada_simétrica(f::Function, x0::Number, h::Number)\n",
    "    \n",
    "    x0 = float(x0)\n",
    "    h = float(h)\n",
    "    \n",
    "    derivada = (f(x0 + h) - f(x0 - h))/(2*h)\n",
    "    \n",
    "    return(derivada)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estudio del error de la derivada numérica simétrica de acuerdo al tamaño del paso**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se usan los mismos puntos muestra que para el estudio del error de la derivada derecha:\n",
    "\n",
    "errores_f₄_derivada_simétrica = []\n",
    "\n",
    "for h_paso in h_muestra\n",
    "    \n",
    "    error = error_derivada(f₄, f₄prime, x0, h_paso, derivada_simétrica)\n",
    "    push!(errores_f₄_derivada_simétrica, error)\n",
    "end\n",
    "\n",
    "gráfica_f₄_derivada_simétrica = plot(h_muestra, errores_f₄_derivada_simétrica, xscale = :log10, legend = false, xlabel = \"h\", ylabel = \"Valor absoluto del error\", title = \"Error de la derivada numérica simétrica de f4 como función del tamaño del paso en x0 = $x0\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una mejora logarítmica, aunque \"rápida\" al inicio (cuando $h \\approx 1$), del error. Este tiene un valor menor a $10^{-6}$ cuando $ h \\lessapprox 5 \\times 10^{-4} $. Sin embargo, en torno a $h \\approx 5 \\times 10^{-16}$, el error aumenta de manera logarítmica para estabilizarse cerca de 10 cuando $h \\lessapprox 10^{-16}$. Esto parece sugerir dos cosas:\n",
    "\n",
    "1. La derivada numérica simétrica converge más rápido que la derivada numérica derecha en el ejemplo, y,\n",
    "2. El error en el que se incurre para $h$ \"muy bajo\" no depende del método, sino de la función o de las limitaciones de la representación de números en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Argumentación sobre cuál método es mejor y por qué**:\n",
    "\n",
    "Para tener una mejor idea sobre lo anterior, conviene realizar una gráfica comparativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfica comparativa de los métodos.\n",
    "\n",
    "gráfica_f₄_comparación = plot(h_muestra, errores_f₄_derivada_derecha, label = \"Derivada derecha\")\n",
    "plot!(gráfica_f₄_comparación, h_muestra, errores_f₄_derivada_simétrica, label = \"Derivada simétrica\", xscale = :log10, legend = :topleft, xlabel = \"h\", ylabel = \"Valor absoluto del error\", title = \"Comparación de los errores de la derivada numérica según el método\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en el ejemplo, el método simétrico converge más rápido que el método derecho. Sin embargo, en torno a $ h = 10^{-8} $ ambos métodos tienen un error menor a $ 10 ^ 6 $. Obsérvese que ambos métodos se comportan de forma similar para $ h $ \"muy bajos\" ($ h \\approx 10^{-16}$: el error aumenta rápidamente. Este orden de magnitud es el mismo que el del épsilon de la máquina, por lo que el aumento en el error podría deberse a las limitaciones de la representación de los flotantes en memoria. \n",
    "\n",
    "*Nota*: Investigando sobre el siguiente ejercicio, se encontró que en [este artículo](http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf) se menciona que el aumento en el error en el que se incurre para $h$ \"pequeños\" se debe a los errores incurridos por \"cancelación vía sustracción\". Al hacer esta afirmación, se cita un libro [Gill et al., 1981] en el que se describe que el \"error de cancelación\" consiste en que dos valores dados, al momento de ser representados como números de punto flotante y tomar su diferencia, si los valores son suficientemente cercanos y debido a la finitud de la precisión de las representaciones, la diferencia resultante (que aproxima la diferencia real) puede un error relativo mucho mayor a un múltiplo \"pequeño\" del épsilon de la máquina. Este error de cancelación ocurre precisamente al momento de tomar la diferencia para calcular el numerador con pasos pequeños, lo que explica tanto la anomalía de los datos en este régimen como el orden de magnitud en el que ocurre.\n",
    "\n",
    "Una forma directa de entender por qué el método simétrico converge, en general, más rápido que el derecho consiste en observar lo siguiente:\n",
    "\n",
    "Sea $f$ una función \"decente\" en el sentido de satisfacer en una vecindad de $x_0$, si $h$ es \"suficientemente pequeño\":\n",
    "\n",
    "$$ f(x_0 \\pm h) = f(x_0) \\pm h f'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\mathcal{O}(h^3), $$\n",
    "\n",
    "dónde $ \\mathcal{O}(h^n) $ es un término de error que tiende a cero conforme $ h \\longrightarrow 0 $, con la rapidez de convergencia dada por el exponente $n$.\n",
    "\n",
    "De esto, si $ f'_+(x_0; h) $ representa la derivada numérica derecha de $ f $ en $x_0$ con tamaño de paso $ h $, se tiene que:\n",
    "\n",
    "$$ f'_+(x_0; h) := \\frac{f(x_0 + h) - f(x_0)}{h} = f'(x_0) + \\frac{h}{2}f''(x_0)+  \\frac{1}{h} \\mathcal{O}(h^3). $$\n",
    "\n",
    "Y si $ f'_{sym}(x_0; h) $ representa la derivada numérica simétrica de $ f $ en $x_0$ con tamaño de paso $ h $:\n",
    "\n",
    "$$ f'_{sym}(x_0; h) := \\frac{f(x_0 + h) - f(x_0 - h)}{2h} = f'(x_0) + \\frac{1}{2h} \\mathcal{O}(h^3). $$\n",
    "\n",
    "Si $ f $ es \"decente\" y $ h < 1 $, entonces, en general:\n",
    "\n",
    "$$ f'_+(x_0; h) = f'(x_0) + \\frac{h}{2}f''(x_0)+  \\frac{1}{h} \\mathcal{O}(h^3) = f'(x_0) + \\mathcal{O}(h), $$\n",
    "\n",
    "y,\n",
    "\n",
    "$$ f'_{sym}(x_0; h) = f'(x_0) + \\mathcal{O}(h^2). $$\n",
    "\n",
    "\n",
    "Por lo que el término del error en el método derecho es en general lineal (salvo en el caso en el que $ f''(x_0) = 0 $, en cuyo caso será cuadrático), mientras que el término del error en el método simétrico es cuadrático en general. Esto quiere decir que el método simétrico converge como el cuadrado del tamaño del paso dado, a diferencia de la convergencia lineal (en general) del método derecho, lo que explica que el método simétrico sea más rápido en converger. La razón por la cual no se aprecia el tipo de convergencia calculado en los ejemplos es por el tipo de puntos muestra usados para las gráficas: no son suficientes para refinar el detalle de la forma de convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "- Definan la función `derivada_compleja` considerando la definición de la derivada dada por:\n",
    "$$\n",
    "f'(x_0) \\approx \\frac{\\Delta f_{cmplx}(x_0)}{\\Delta x} = \\Im\\Big(\\frac{f(x_0+ i h)}{h}\\Big),\n",
    "$$\n",
    "donde $\\Im$ indica la parte imaginaria del argumento ($i=\\sqrt{-1}$). \n",
    "\n",
    "- Argumenta (analíticamente) por qué esta definición da tan buen resultado. ¿Hay algún caso en que esta definición puede dar resultados inesperados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    derivada_compleja(f, x0, h)\n",
    "\n",
    "`derivada_compleja` es una función que calcula la derivada numérica compleja de la función dada en el punto suministrado con el tamaño de paso definido.\n",
    "\n",
    "# Definición\n",
    "\n",
    "La derivada numérica compleja de `f` en el punto `x0` con tamaño de paso `h` está dada por: \n",
    "\n",
    "\\$ f'_{cmplx} (x_0; h) = \\Im \\left( \\frac{f(x_0 + ih)}{h} \\right). \\$\n",
    "\n",
    "# Argumentos\n",
    "\n",
    "Para poder utilizar la función se requieren los siguientes argumentos:\n",
    "\n",
    "* `f`, la función (real) de variable (real) de la que se quiere calcular la derivada,\n",
    "* `x0`, el punto en el que se desea calcular la derivada, y,\n",
    "* `h`, el tamaño de paso usado para aproximar la derivada exacta.\n",
    "\n",
    "`derivada_compleja` requiere que `f` sea una función, y que `x0` y `h` sean números. En particular, para mejorar la estabilidad de tipo, `x0` y `h` son convertidos a números flotantes de 64 bits.\n",
    "\n",
    "# Ejemplos:\n",
    "```julia-repl\n",
    "    julia> derivada_compleja(x -> x, 1, 0.01)\n",
    "    1.0\n",
    "\n",
    "    julia> derivada_compleja(x -> x^2, 1, 0.01)\n",
    "    2.0\n",
    "\n",
    "    julia> derivada_compleja(x -> sin(x), 0, 0.001)\n",
    "    1.0000001666666751\n",
    "\n",
    "    julia> derivada_compleja(x -> exp(x), 0, 0.001)\n",
    "    0.9999998333333416\n",
    "```\n",
    "\n",
    "# Notas:\n",
    "\n",
    "*Nota*: Por la definición de la derivada numérica compleja, esta sólo funciona para funciones reales que puedan ser extendidas al plano complejo con una función analítica en una vecindad del punto de interés. Esta función no aproxima adecuadamente la derivada de una función compleja.\n",
    "\"\"\"\n",
    "function derivada_compleja(f::Function, x0::Number, h::Number)\n",
    "\n",
    "    x0 = float(x0)\n",
    "    h = float(h)\n",
    "    \n",
    "    derivada = f(x0 + h*im)/ h\n",
    "    \n",
    "    return(derivada.im)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estudio del error de la derivada numérica compleja de acuerdo al tamaño del paso**:\n",
    "\n",
    "Para completar esto, conviene realizar un estudio de la dependencia del error respecto al tamaño del paso similar a los realizados anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se usan los mismos puntos muestra que para el estudio del error de la derivada derecha:\n",
    "\n",
    "errores_f₄_derivada_compleja = []\n",
    "\n",
    "for h_paso in h_muestra\n",
    "    \n",
    "    error = error_derivada(f₄, f₄prime, x0, h_paso, derivada_compleja)\n",
    "    push!(errores_f₄_derivada_compleja, error)\n",
    "end\n",
    "\n",
    "gráfica_f₄_derivada_compleja = plot(h_muestra, errores_f₄_derivada_compleja, xscale = :log10, legend = false, xlabel = \"h\", ylabel = \"Valor absoluto del error\", title = \"Error de la derivada numérica compleja de f4 como función del tamaño del paso en x0 = $x0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una mejora logarítmica, aunque \"rápida\" al inicio (cuando $h \\approx 1$), del error. Este tiene un valor menor a $10^{-6}$ cuando $ h \\lessapprox 5 \\times 10^{-4} $. No existe el comportamiento anómalo observado en los ejemplos anteriores para $h$ pequeña. Esto, según lo comentado por el artículo mencionado en el ejercicio 5 ([Este](http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf)), se debe a que no se incurre en error de cancelación al no tomarse diferencia alguna.\n",
    "\n",
    "Para ver esto de forma más explícita se pueden graficar los resultados de lar tres definiciones de la derivada numérica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfica comparativa de los métodos.\n",
    "\n",
    "#Agregando los últimos datos a la comparación anterior:\n",
    "plot!(gráfica_f₄_comparación, h_muestra, errores_f₄_derivada_compleja, label = \"Derivada compleja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa, para el ejemplo, la derivada compleja converge aparentemente tan rápido como la derivada simétrica, aunque no presenta el error para $h \\approx 10^{-16}$, lo que parece confirmar lo mencionado en la referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentario sobre el por qué el método funciona**:\n",
    "\n",
    "Para entender por qué el método funciona tan bien, considérese el siguiente planteamiento, derivado de [este artículo](http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf) y [este post](https://timvieira.github.io/blog/post/2014/08/07/complex-step-derivative/):\n",
    "\n",
    "Sea $ f: \\mathbb{R} \\rightarrow \\mathbb{R} $ una función diferenciable en una vecindad $V$ de $ x_0 \\in \\mathbb{R}$. Sea $F: A \\subseteq \\mathbb{C}$ la \"complejificación\" de $ f $ en el sentido de que $F|_U(X) = f(x)$ con $U = V \\times \\{0\\} \\subset \\mathbb{C}$ y $X = (x,0) \\in \\mathbb{C} $; entonces, si $F$ es analítica en el sentido complejo en $ A $ y $u: \\mathbb{C} \\rightarrow \\mathbb{R} $ y $v: \\mathbb{C} \\rightarrow \\mathbb{R} $ con $F(x + iy) = u(x + iy) + i v(x + iy)$, se tiene que para todo punto $z \\in U$ deben de satisfacerse las ecuaciones de Riemann-Cauchy:\n",
    "\n",
    "$$ \\frac{\\partial u}{\\partial x}(z) = \\frac{\\partial v}{\\partial y}(z), $$\n",
    "\n",
    "y, \n",
    "\n",
    "$$ \\frac{\\partial v}{\\partial x}(z) = -\\frac{\\partial u}{\\partial y}(z). $$\n",
    "\n",
    "(Es decir, $F$ es una función compleja y analítica en su dominio tal que su restricción a un intervalo de la recta real que contiene a $x_0$ es igual a $f$. Por ser $F$ analítica, todo punto en su dominio satisface las ecuaciones de Riemann-Cauchy.)\n",
    "\n",
    "De esto, como $f$ es una función real de variable real, si $ z = x + 0i \\in U$: $f(x) = F(z) = u(x) + v(x) = u(x)$, es decir: $v(x) = 0$. \n",
    "\n",
    "\n",
    "En particular, si $ x_0 \\in V$ y  $ z = x_0 + 0i \\in U$:\n",
    "\n",
    "$$ f'(x_0) = \\frac{\\partial u}{\\partial x} (x_0) = \\frac{\\partial v}{\\partial y}(x_0) = \\lim_{h \\rightarrow 0} \\frac{v(x_0 + ih) - v(x_0) }{\\partial h} = \\lim_{h \\rightarrow 0} \\frac{v(x_0 + ih) }{h} = \\lim_{h \\rightarrow 0} \\Im \\left( \\frac{F(x_0 + ih) }{h} \\right).$$\n",
    "\n",
    "Obsérvese que $h \\in \\mathbb{R}.$\n",
    "\n",
    "Esto permite entender por qué la derivada numérica compleja es definida de la siguiente forma:\n",
    "\n",
    "$$ f'_{cmplx} (x_0; h) = \\Im \\left( \\frac{f(x_0 + ih)}{h} \\right). $$\n",
    "\n",
    "Y en específico, por qué esta no presenta los problemas que las otras derivadas: porque no involucra sustracciones que lleven a errores de cancelación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma sencilla de dar cotas de convergencia para el método, es analizar la complejificación de la función dada. Dado que $F$ es analítica en un dominio $A$ que contiene al punto real de interés, denotando la derivada compleja de $F$ mediante $\\frac{\\text{d} F}{\\text{d}z} $:\n",
    "\n",
    "$$ F(x_0 + i h) = F(x_0) + i h \\frac{\\text{d} F}{\\text{d}z}(x_0) + \\mathcal{O}(h^2),$$\n",
    "\n",
    "de dónde, al ser $v(x) = 0$ y $f(x) = u(x)$ en la recta real, para $h$ \"pequeño\":\n",
    "\n",
    "$$ F(x_0 + i h) = f(x_0) + i h f'(x_0) + \\frac{(ih)^2}{2!} f''(x_0) + \\frac{(ih)^3}{3!} f'''(x_0) + \\mathcal{O}(h^4)\\\\\n",
    "    = f(x_0) + i h f'(x_0) - \\frac{h^2}{2!} f''(x_0) - \\frac{ih^3}{3!} f'''(x_0) + \\mathcal{O}(h^4).\n",
    "$$\n",
    "\n",
    "Reordenando:\n",
    "\n",
    "$$\n",
    "   F(x_0 + i h) = f(x_0) - \\frac{h^2}{2!} f''(x_0) + i \\big[ h f'(x_0) - \\frac{h^3}{3!} f'''(x_0) \\big] + \\mathcal{O}(h^4),\n",
    "$$\n",
    "\n",
    "por lo que:\n",
    "\n",
    "$$ \\Im \\left( \\frac{F(x_0 + i h)}{h} \\right) = f'(x_0) - \\frac{h^2}{3!} f'''(x_0) + \\mathcal{O}(h^3).  $$\n",
    "\n",
    "De esta última expresión, se tiene:\n",
    "\n",
    "$$ f'(x_0) = \\Im \\left( \\frac{F(x_0 + i h)}{h} \\right) + \\frac{h^2}{3!} f'''(x_0) + \\mathcal{O}(h^3) = \\Im \\left( \\frac{F(x_0 + i h)}{h} \\right) + \\mathcal{O}(h^2) = f'_{cmplx} (x_0; h) + \\mathcal{O}(h^2),  $$\n",
    "\n",
    "por lo que la expresión de la derivada numérica compleja tiene un error que va como el cuadrado del tamaño del paso respecto del valor analítico. Esto explica el por qué el error de este método es similar al error del método simétrico para $h \\approx 1$. La razón por la cual no se aprecia directamente esto en la gráfica es la misma que en los casos anteriores: no hay suficientes puntos para observar esto con detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sobre las limitaciones de la derivada numérica compleja**:\n",
    "\n",
    "En el artículo se menciona que funciones que no tienen una complejificación analítica pueden dar resultados anómalos. Un ejemplo de esto (al menos con la extensión estándar) es la función valor absoluto. \n",
    "\n",
    "Sea $ z = x + iy$. La extensión de la función valor absoluto dada por $|z|_1 = \\sqrt{ x^2 + y^2}$ es una función real de variable compleja, por lo que la parte imaginaria de esta extensión es idénticamente cero. Esta extensión no es analítica debido a que no satisface las ecuaciones de Riemann-Cauchy. (La parcial de la función respecto de x no es idénticamente cero.)\n",
    "\n",
    "Observemos lo que sucede al usar esta definición de la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es necesario (re)definir la función valor absoluto para poder obtener un número complejo (aunque con parte compleja cero) como resultado:\n",
    "\n",
    "absoluto_1(z) = Complex(abs(z))\n",
    "\n",
    "#Calculando la derivada numérica compleja de la función en x0 = 1:\n",
    "\n",
    "derivada_compleja(absoluto_1, 1, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, la derivada numérica compleja arroja un resultado equivocado en este ejemplo.\n",
    "\n",
    "Una forma de poder usar este método para esta función, es definir la siguiente complejificación de la función valor absoluto (dada en el artículo), que satisface ser analítica (por partes) en el sentido complejo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo la complejificación analítica:\n",
    "\n",
    "function absoluto_2(z)\n",
    "    \n",
    "    z = Complex{Float64}(z)\n",
    "    \n",
    "    x = z.re\n",
    "    y = z.im\n",
    "    \n",
    "    if x < 0\n",
    "        return(-z)\n",
    "        \n",
    "    else x > 0\n",
    "        return(z)\n",
    "    end\n",
    "end\n",
    "\n",
    "#Calculando la derivada numérica compleja de la función en x0 = 1:\n",
    "\n",
    "derivada_compleja(absoluto_2, 1, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se obtiene el resultado correcto.\n",
    "\n",
    "Otras funciones en las que la derivada numérica compleja no dará resultados adecuados son funciones que arrojen resultados complejos en el punto en el que se quiere calcular la derivada, esto debido a que en la deducción se requirió que el valor resultante de la función sea real. Un ejemplo de esto es tratar de calcular la derivada de la función identidad en el punto $ z = i $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivada_compleja(z -> z, im, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, el resultado es incorrecto."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "julia-0.6"
  },
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  },
  "nteract": {
   "version": "0.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
